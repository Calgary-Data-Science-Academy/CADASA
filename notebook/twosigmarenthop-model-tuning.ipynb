{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "from sklearn import preprocessing, pipeline, metrics, model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_json('../input/two-sigma-connect-rental-listing-inquiries/train.json.zip', convert_dates=['created'])\n",
    "test_data = pd.read_json('../input/two-sigma-connect-rental-listing-inquiries/test.json.zip', convert_dates=['created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create target variables\n",
    "\n",
    "We need to convert the raw target variable into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target'] = train_data['interest_level'].apply(lambda x: 0 if x=='low' else 1 if x=='medium' else 2)\n",
    "train_data['low'] = train_data['interest_level'].apply(lambda x: 1 if x=='low' else 0)\n",
    "train_data['medium'] = train_data['interest_level'].apply(lambda x: 1 if x=='medium' else 0)\n",
    "train_data['high'] = train_data['interest_level'].apply(lambda x: 1 if x=='high' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge training and testing data\n",
    "So we don't have to perform transformations twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data=pd.concat([train_data,test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = ['bathrooms','bedrooms','latitude','longitude','price']\n",
    "cat_vars = ['building_id','manager_id','display_address','street_address']\n",
    "text_vars = ['description','features']\n",
    "date_var = 'created'\n",
    "image_var = 'photos'\n",
    "id_var = 'listing_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date/time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['created_datetime'] = pd.to_datetime(full_data['created'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "full_data['created_year']=full_data['created_datetime'].apply(lambda x:x.year) ## low variant\n",
    "full_data['created_datetime'] = pd.to_datetime(full_data['created'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "full_data['created_month']=full_data['created_datetime'].apply(lambda x:x.month)\n",
    "full_data['created_day']=full_data['created_datetime'].apply(lambda x:x.day)\n",
    "full_data['created_dayofweek']=full_data['created_datetime'].apply(lambda x:x.dayofweek)\n",
    "full_data['created_dayofyear']=full_data['created_datetime'].apply(lambda x:x.dayofyear)\n",
    "full_data['created_weekofyear']=full_data['created_datetime'].apply(lambda x:x.weekofyear)\n",
    "full_data['created_hour']=full_data['created_datetime'].apply(lambda x:x.hour)\n",
    "full_data['created_epoch']=full_data['created_datetime'].apply(lambda x:x.value//10**9)\n",
    "\n",
    "date_num_vars = ['created_month','created_dayofweek','created_dayofyear'\n",
    "                 ,'created_weekofyear','created_hour','created_epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geolocation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"geo_area_50\"] = \\\n",
    "    full_data[['latitude', 'longitude']]\\\n",
    "    .apply(lambda x:(int(x[0]*50)%50)*50+(int(-x[1]*50)%50),axis=1)                                         \n",
    "                         \n",
    "\n",
    "full_data[\"geo_area_100\"] = \\\n",
    "    full_data[['latitude', 'longitude']]\\\n",
    "    .apply(lambda x:(int(x[0]*100)%100)*100+(int(-x[1]*100)%100),axis=1)                                         \n",
    "  \n",
    "\n",
    "full_data[\"geo_area_200\"] = \\\n",
    "    full_data[['latitude', 'longitude']]\\\n",
    "    .apply(lambda x:(int(x[0]*200)%200)*200+(int(-x[1]*200)%200),axis=1)                                         \n",
    "\n",
    "import math\n",
    "\n",
    "# Financial district\n",
    "lat=40.705628\n",
    "lon=-74.010278\n",
    "full_data['distance_to_fi'] = full_data[['latitude', 'longitude']].apply(lambda x:math.sqrt((x[0]-lat)**2+(x[1]-lon)**2), axis=1)\n",
    "\n",
    "# Central park\n",
    "lat = 40.785091\n",
    "lon = -73.968285\n",
    "full_data['distance_to_cp'] = full_data[['latitude', 'longitude']].apply(lambda x:math.sqrt((x[0]-lat)**2+(x[1]-lon)**2), axis=1)\n",
    "\n",
    "\n",
    "geo_cat_vars = ['geo_area_50', 'geo_area_100', 'geo_area_200']\n",
    "\n",
    "geo_num_vars = ['distance_to_fi', 'distance_to_cp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric features: basic engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['rooms'] = full_data['bedrooms'] + full_data['bathrooms'] \n",
    "full_data['num_of_photos'] = full_data['photos'].apply(lambda x:len(x))\n",
    "full_data['num_of_features'] = full_data['features'].apply(lambda x:len(x))\n",
    "full_data['len_of_desc'] = full_data['description'].apply(lambda x:len(x))\n",
    "full_data['words_of_desc'] = full_data['description'].apply(lambda x:len(re.sub('['+string.punctuation+']', '', x).split()))\n",
    "\n",
    "\n",
    "full_data['nums_of_desc'] = full_data['description']\\\n",
    "        .apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n",
    "        .apply(lambda x: len([s for s in x if s.isdigit()]))\n",
    "        \n",
    "full_data['has_phone'] = full_data['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n",
    "        .apply(lambda x: [s for s in x if s.isdigit()])\\\n",
    "        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n",
    "        .apply(lambda x: 1 if x>0 else 0)\n",
    "full_data['has_email'] = full_data['description'].apply(lambda x: 1 if '@renthop.com' in x else 0)\n",
    "\n",
    "full_data['building_id_is_zero'] = full_data['building_id'].apply(lambda x:1 if x=='0' else 0)\n",
    "\n",
    "additional_num_vars = ['rooms','num_of_photos','num_of_features','len_of_desc',\n",
    "                    'words_of_desc','has_phone','has_email','building_id_is_zero']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric-Numeric interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['avg_word_len'] = full_data[['len_of_desc','words_of_desc']]\\\n",
    "                                    .apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "    \n",
    "full_data['price_per_room'] = full_data[['price','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_bedroom'] = full_data[['price','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_bathroom'] = full_data[['price','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_feature'] = full_data[['price','num_of_features']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_photo'] = full_data[['price','num_of_photos']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_word'] = full_data[['price','words_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_by_desc_len'] = full_data[['price','len_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "\n",
    "\n",
    "full_data['photos_per_room'] = full_data[['num_of_photos','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['photos_per_bedroom'] = full_data[['num_of_photos','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['photos_per_bathroom'] = full_data[['num_of_photos','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "\n",
    "full_data['desc_len_per_room'] = full_data[['len_of_desc','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['desc_len_per_bedroom'] = full_data[['len_of_desc','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['desc_len_per_bathroom'] = full_data[['len_of_desc','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['desc_len_per_word'] = full_data[['len_of_desc','words_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['desc_len_per_numeric'] = full_data[['len_of_desc','nums_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "\n",
    "full_data['features_per_room'] = full_data[['num_of_features','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['features_per_bedroom'] = full_data[['num_of_features','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['features_per_bathroom'] = full_data[['num_of_features','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['features_per_photo'] = full_data[['num_of_features','num_of_photos']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['features_per_word'] = full_data[['num_of_features','words_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['features_by_desc_len'] = full_data[['num_of_features','len_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "\n",
    "\n",
    "interactive_num_vars = ['avg_word_len','price_per_room','price_per_bedroom','price_per_bathroom',\n",
    "                        'price_per_feature','price_per_photo','price_per_word','price_by_desc_len',\n",
    "                        'photos_per_room','photos_per_bedroom','photos_per_bathroom',\n",
    "                        'desc_len_per_room','desc_len_per_bedroom','desc_len_per_bathroom','desc_len_per_word',\n",
    "                        'desc_len_per_numeric','features_per_room','features_per_bedroom','features_per_bathroom',\n",
    "                        'features_per_photo','features_per_word','features_by_desc_len']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "display=full_data[\"display_address\"].value_counts()\n",
    "manager_id=full_data[\"manager_id\"].value_counts()\n",
    "building_id=full_data[\"building_id\"].value_counts()\n",
    "street=full_data[\"street_address\"].value_counts()\n",
    "bedrooms=full_data[\"bedrooms\"].value_counts()\n",
    "bathrooms=full_data[\"bathrooms\"].value_counts()\n",
    "created_dayofyear=full_data[\"created_dayofyear\"].value_counts()\n",
    "created_weekofyear=full_data[\"created_weekofyear\"].value_counts()\n",
    "\n",
    "full_data[\"display_count\"]=full_data[\"display_address\"].apply(lambda x:display[x])\n",
    "full_data[\"manager_count\"]=full_data[\"manager_id\"].apply(lambda x:manager_id[x])  \n",
    "full_data[\"building_count\"]=full_data[\"building_id\"].apply(lambda x:building_id[x])\n",
    "full_data[\"street_count\"]=full_data[\"street_address\"].apply(lambda x:street[x])\n",
    "full_data[\"bedrooms_count\"]=full_data[\"bedrooms\"].apply(lambda x:bedrooms[x])\n",
    "full_data[\"bathrooms_count\"]=full_data[\"bathrooms\"].apply(lambda x:bathrooms[x])\n",
    "full_data[\"created_dayofyear_count\"]=full_data[\"created_dayofyear\"].\\\n",
    "    apply(lambda x:created_dayofyear[x])\n",
    "full_data[\"created_weekofyear_count\"]=full_data[\"created_weekofyear\"].\\\n",
    "    apply(lambda x:created_weekofyear[x])\n",
    "\n",
    "count_vars = ['manager_count', 'building_count', 'street_count', 'bedrooms_count',\n",
    "       'bathrooms_count', 'created_dayofyear_count', 'created_weekofyear_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric-categorical interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cat_vars =[]\n",
    "price_by_manager = full_data.groupby('manager_id')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_manager.columns = ['manager_id','min_price_by_manager',\n",
    "                            'max_price_by_manager','median_price_by_manager','mean_price_by_manager']\n",
    "full_data = pd.merge(full_data,price_by_manager, how='left',on='manager_id')\n",
    "\n",
    "price_by_building = full_data.groupby('building_id')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_building.columns = ['building_id','min_price_by_building',\n",
    "                            'max_price_by_building','median_price_by_building','mean_price_by_building']\n",
    "full_data = pd.merge(full_data,price_by_building, how='left',on='building_id')\n",
    "\n",
    "\n",
    "full_data['price_percentile_by_manager']=\\\n",
    "            full_data[['price','min_price_by_manager','max_price_by_manager']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n",
    "full_data['price_percentile_by_building']=\\\n",
    "            full_data[['price','min_price_by_building','max_price_by_building']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n",
    "\n",
    "\n",
    "num_cat_vars.append('price_percentile_by_manager')\n",
    "num_cat_vars.append('price_percentile_by_building')\n",
    "\n",
    "print (num_cat_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-way categorical features interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comb in itertools.combinations(cat_vars, 2):\n",
    "    comb_var_name = comb[0] +'-'+ comb[1]\n",
    "    full_data [comb_var_name] = full_data [ comb[0]].astype(str) +'_' + full_data [ comb[1]].astype(str)\n",
    "    cat_vars.append(comb_var_name)\n",
    "\n",
    "cat_vars    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text features\n",
    "\n",
    "* Here we are using CountVectorizer but you are encouraged to give TfidfVectorizer a try.\n",
    "\n",
    "* The parameter of max_features to be tuned\n",
    "\n",
    "* The outputs are sparse matrices which can be merged with numpy arrays using scipy.stats.sparse.hstack function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "cntvec = CountVectorizer(stop_words='english', max_features=200)\n",
    "feature_sparse =cntvec.fit_transform(full_data[\"features\"]\\\n",
    "                                     .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x])))\n",
    "\n",
    "feature_vars = ['feature_' + v for v in cntvec.vocabulary_]\n",
    "\n",
    "cntvec = CountVectorizer(stop_words='english', max_features=100)\n",
    "desc_sparse = cntvec.fit_transform(full_data[\"description\"])\n",
    "desc_vars = ['desc_' + v for v in cntvec.vocabulary_]\n",
    "\n",
    "\n",
    "cntvec = CountVectorizer(stop_words='english', max_features=10)\n",
    "st_addr_sparse = cntvec.fit_transform(full_data[\"street_address\"])\n",
    "st_addr_vars = ['desc_' + v for v in cntvec.vocabulary_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features - label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBL = preprocessing.LabelEncoder()\n",
    "\n",
    "LE_vars=[]\n",
    "LE_map=dict()\n",
    "for cat_var in cat_vars:\n",
    "    print (\"Label Encoding %s\" % (cat_var))\n",
    "    LE_var=cat_var+'_le'\n",
    "    full_data[LE_var]=LBL.fit_transform(full_data[cat_var])\n",
    "    LE_vars.append(LE_var)\n",
    "    LE_map[cat_var]=LBL.classes_\n",
    "    \n",
    "print (\"Label-encoded feaures: %s\" % (LE_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features - one hot encoding\n",
    "\n",
    "The output is a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "start=time.time()\n",
    "OHE.fit(full_data[LE_vars])\n",
    "OHE_sparse=OHE.transform(full_data[LE_vars])\n",
    "                                   \n",
    "print ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n",
    "\n",
    "\n",
    "OHE_vars = [var[:-3] + '_' + str(level).replace(' ','_')\\\n",
    "                for var in cat_vars for level in LE_map[var] ]\n",
    "\n",
    "print (\"OHE_sparse size :\" ,OHE_sparse.shape)\n",
    "print (\"One-hot encoded catgorical feature samples : %s\" % (OHE_vars[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features - mean encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product\n",
    "\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    "\n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    "\n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    "\n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \"\"\"\n",
    "\n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    "\n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    "\n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    "\n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    "\n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({'mean': 'mean', 'beta': 'size'})\n",
    "        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n",
    "        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n",
    "\n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    "\n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_encoder = MeanEncoder(categorical_features=['manager_id','building_id'])\n",
    "mean_encoded_train = mean_encoder.fit_transform(train_data, train_data['target'])\n",
    "mean_encoded_test = mean_encoder.transform(test_data)\n",
    "\n",
    "mean_coded_vars = list(set(mean_encoded_train.columns) - set(train_data.columns))\n",
    "mean_coded_vars.append('listing_id')\n",
    "full_data = pd.merge(full_data, \n",
    "                     pd.concat([mean_encoded_train[mean_coded_vars], mean_encoded_test[mean_coded_vars]]),\n",
    "                     how='left',\n",
    "                     on='listing_id'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars +geo_num_vars+ count_vars + LE_vars + mean_coded_vars\n",
    "train_x = sparse.hstack([full_data[full_vars], \n",
    "                         feature_sparse, \n",
    "                         desc_sparse, \n",
    "                         st_addr_sparse]).tocsr()[:train_size]\n",
    "train_y = full_data['target'][:train_size].values\n",
    "test_x = sparse.hstack([full_data[full_vars], \n",
    "                        feature_sparse, \n",
    "                        desc_sparse, \n",
    "                        st_addr_sparse]).tocsr()[train_size:]\n",
    "test_y = full_data['target'][train_size:].values\n",
    "\n",
    "\n",
    "full_vars = full_vars + feature_vars + desc_vars + st_addr_vars    \n",
    "print (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', 'created_month', 'created_dayofweek', 'created_dayofyear', 'created_weekofyear', 'created_hour', 'created_epoch', 'rooms', 'num_of_photos', 'num_of_features', 'len_of_desc', 'words_of_desc', 'has_phone', 'has_email', 'building_id_is_zero', 'avg_word_len', 'price_per_room', 'price_per_bedroom', 'price_per_bathroom', 'price_per_feature', 'price_per_photo', 'price_per_word', 'price_by_desc_len', 'photos_per_room', 'photos_per_bedroom', 'photos_per_bathroom', 'desc_len_per_room', 'desc_len_per_bedroom', 'desc_len_per_bathroom', 'desc_len_per_word', 'desc_len_per_numeric', 'features_per_room', 'features_per_bedroom', 'features_per_bathroom', 'features_per_photo', 'features_per_word', 'features_by_desc_len', 'geo_area_50', 'geo_area_100', 'geo_area_200', 'manager_count', 'building_count', 'street_count', 'bedrooms_count', 'bathrooms_count', 'created_dayofyear_count', 'created_weekofyear_count', 'building_id_le', 'manager_id_le', 'display_address_le', 'street_address_le', 'building_id-manager_id_le', 'building_id-display_address_le', 'building_id-street_address_le', 'manager_id-display_address_le', 'manager_id-street_address_le', 'display_address-street_address_le', 'building_id_pred_1', 'manager_id_pred_2', 'building_id_pred_0', 'building_id_pred_2', 'manager_id_pred_0', 'manager_id_pred_1', 'listing_id'])-set(['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', 'created_month', 'created_dayofweek', 'created_dayofyear', 'created_weekofyear', 'created_hour', 'created_epoch', 'rooms', 'num_of_photos', 'num_of_features', 'len_of_desc', 'words_of_desc', 'has_phone', 'has_email', 'building_id_is_zero', 'avg_word_len', 'price_per_room', 'price_per_bedroom', 'price_per_bathroom', 'price_per_feature', 'price_per_photo', 'price_per_word', 'price_by_desc_len', 'photos_per_room', 'photos_per_bedroom', 'photos_per_bathroom', 'desc_len_per_room', 'desc_len_per_bedroom', 'desc_len_per_bathroom', 'desc_len_per_word', 'desc_len_per_numeric', 'features_per_room', 'features_per_bedroom', 'features_per_bathroom', 'features_per_photo', 'features_per_word', 'features_by_desc_len', 'norm_listing_id', 'building_id_le', 'manager_id_le', 'display_address_le', 'street_address_le', 'building_id-manager_id_le', 'building_id-display_address_le', 'building_id-street_address_le', 'manager_id-display_address_le', 'manager_id-street_address_le', 'display_address-street_address_le', 'building_id_pred_1', 'manager_id_pred_2', 'building_id_pred_0', 'building_id_pred_2', 'manager_id_pred_0', 'manager_id_pred_1', 'listing_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM \n",
    "\n",
    "Typically, GBDT model converges faster with a larger learning rate (e.g 0.1) than smaller learning rate however the accuracy may not be as promising. We will be using 0.1 as the learning rate for the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import lightgbm as lgb\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.1\n",
    "lgb_params['num_leaves'] = 63\n",
    "lgb_params['max_depth'] = 15\n",
    "lgb_params['min_gain_to_split '] = 1\n",
    "lgb_params['subsample'] = 0.7\n",
    "lgb_params['colsample_bytree'] = 0.7\n",
    "lgb_params['min_sum_hessian_in_leaf'] = 0.001\n",
    "lgb_params['seed']=42\n",
    "\n",
    "lgb_cv = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(train_x,\n",
    "                            label=train_y\n",
    "                            ),\n",
    "                num_boost_round=100000,\n",
    "                nfold=5,\n",
    "                stratified=True,\n",
    "                shuffle=True,\n",
    "                early_stopping_rounds=50,\n",
    "                seed=42,\n",
    "                verbose_eval=50)\n",
    "\n",
    "\n",
    "best_score = min(lgb_cv['multi_logloss-mean'])\n",
    "best_iteration = len(lgb_cv['multi_logloss-mean'])\n",
    "print ('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n",
    "# 0.549718"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.05\n",
    "lgb_params['num_leaves'] = 63\n",
    "lgb_params['max_depth'] = 15\n",
    "lgb_params['min_gain_to_split '] = 1\n",
    "lgb_params['subsample'] = 0.7\n",
    "lgb_params['colsample_bytree'] = 0.7\n",
    "lgb_params['min_sum_hessian_in_leaf'] = 0.001\n",
    "lgb_params['seed']=42\n",
    "\n",
    "lgb_cv = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(train_x,\n",
    "                            label=train_y\n",
    "                            ),\n",
    "                num_boost_round=100000,\n",
    "                nfold=5,\n",
    "                stratified=True,\n",
    "                shuffle=True,\n",
    "                early_stopping_rounds=50,\n",
    "                seed=42,\n",
    "                verbose_eval=50)\n",
    "\n",
    "\n",
    "best_score = min(lgb_cv['multi_logloss-mean'])\n",
    "best_iteration = len(lgb_cv['multi_logloss-mean'])\n",
    "print ('Best iteration: %d, best score: %f' % (best_iteration, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## listing ID\n",
    "Theoretically ID variable is not supposed to be included in training a model. However, for some reason listing_id appears to be correlated to the created date time and therefore might be a good candidate as a feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('listing_id', 'created_epoch', full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_listing_id = full_data['listing_id'].min()\n",
    "max_listing_id = full_data['listing_id'].max()\n",
    "full_data['norm_listing_id']=full_data['listing_id'].apply(lambda x:np.float64((x-min_listing_id+1))/(max_listing_id-min_listing_id+1))\n",
    "listing_vars = [ 'norm_listing_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars +geo_num_vars + count_vars \\\n",
    "     + listing_vars\n",
    "full_cat_vars = LE_vars + mean_coded_vars\n",
    "full_vars = full_num_vars + full_cat_vars\n",
    "train_x = sparse.hstack([full_data[full_vars], \n",
    "                         feature_sparse, \n",
    "                         desc_sparse, \n",
    "                         st_addr_sparse]).tocsr()[:train_size]\n",
    "train_y = full_data['target'][:train_size].values\n",
    "test_x = sparse.hstack([full_data[full_vars], \n",
    "                        feature_sparse, \n",
    "                        desc_sparse, \n",
    "                        st_addr_sparse]).tocsr()[train_size:]\n",
    "test_y = full_data['target'][train_size:].values\n",
    "\n",
    "\n",
    "full_vars = full_vars + feature_vars + desc_vars + st_addr_vars    \n",
    "print (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.1\n",
    "lgb_params['num_leaves'] = 63\n",
    "lgb_params['max_depth'] = 15\n",
    "lgb_params['min_gain_to_split '] = 1\n",
    "lgb_params['subsample'] = 0.7\n",
    "lgb_params['colsample_bytree'] = 0.7\n",
    "lgb_params['min_sum_hessian_in_leaf'] = 0.001\n",
    "lgb_params['seed']=42\n",
    "\n",
    "lgb_cv = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(train_x,\n",
    "                            label=train_y\n",
    "                            ),\n",
    "                num_boost_round=100000,\n",
    "                nfold=5,\n",
    "                stratified=True,\n",
    "                shuffle=True,\n",
    "                early_stopping_rounds=50,\n",
    "                seed=42,\n",
    "                verbose_eval=50)\n",
    "\n",
    "\n",
    "best_score = min(lgb_cv['multi_logloss-mean'])\n",
    "best_iteration = len(lgb_cv['multi_logloss-mean'])\n",
    "print ('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.05\n",
    "lgb_params['num_leaves'] = 63\n",
    "lgb_params['max_depth'] = 15\n",
    "lgb_params['min_gain_to_split '] = 1\n",
    "lgb_params['subsample'] = 0.7\n",
    "lgb_params['colsample_bytree'] = 0.7\n",
    "lgb_params['min_sum_hessian_in_leaf'] = 0.001\n",
    "lgb_params['seed']=42\n",
    "\n",
    "lgb_cv = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(train_x,\n",
    "                            label=train_y\n",
    "                            ),\n",
    "                num_boost_round=100000,\n",
    "                nfold=5,\n",
    "                stratified=True,\n",
    "                shuffle=True,\n",
    "                early_stopping_rounds=50,\n",
    "                seed=42,\n",
    "                verbose_eval=50)\n",
    "\n",
    "\n",
    "best_score = min(lgb_cv['multi_logloss-mean'])\n",
    "best_iteration = len(lgb_cv['multi_logloss-mean'])\n",
    "print ('Best iteration: %d, best score: %f' % (best_iteration, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_price = full_data.groupby(['building_id', 'display_address', 'bedrooms', 'bathrooms']).price.mean().reset_index()\n",
    "mkt_price = pd.merge(full_data[['building_id', 'display_address', 'bedrooms', 'bathrooms']],\n",
    "                     mkt_price, how='left', on=['building_id', 'display_address', 'bedrooms', 'bathrooms']).price\n",
    "full_data['mkt_price'] = mkt_price.values\n",
    "full_data['diff_to_mkt_price'] = full_data['price'] - full_data['mkt_price']\n",
    "full_data['ratio_to_mkt_price'] = full_data['price'] / full_data['mkt_price']\n",
    "\n",
    "price_vars = ['diff_to_mkt_price', 'ratio_to_mkt_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hack \"HopScore\"\n",
    "\n",
    "Though it may not be 100% correlated it turns out that Renthop uses a system called [\"**HopScore**\"](https://www.renthop.com/agent-guide/the-hopscore) to rank listings. According to the official instruction there are three things to consider to improve HopScore:\n",
    "\n",
    "* Listing freshness\n",
    "* Listing quality\n",
    "* Manager performance\n",
    "\n",
    "This finding is a breakthrough when I worked on feature engineering for this competition and resulted quite a few fruitful ideas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing freshness and listing quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "# unique identifer for listings - photo links uniquely identify a listing\n",
    "full_data['photos_str'] = full_data['photos'].astype(str)\n",
    "full_data['listing_uid'] = full_data[['manager_id', 'building_id','photos_str']].apply(lambda x: hashlib.md5((x[0] + x[1] + x[2]).encode()).hexdigest(), axis=1 )\n",
    "full_data['posted_times'] = full_data.groupby('listing_uid').created_datetime.rank(method='first', na_option='top',pct=True)\n",
    "\n",
    "# Using html tag may improve listing quality\n",
    "full_data['num_of_html_tag']=full_data.description.apply(lambda x:x.count('<'))\n",
    "\n",
    "# Studies have shown that titles with excessive all caps and special characters give renters the impression \n",
    "# that the listing is fraudulent – i.e. BEAUTIFUL***APARTMENT***CHELSEA.\n",
    "full_data['num_of_#']=full_data.description.apply(lambda x:x.count('#'))\n",
    "full_data['num_of_!']=full_data.description.apply(lambda x:x.count('!'))\n",
    "full_data['num_of_$']=full_data.description.apply(lambda x:x.count('$'))\n",
    "full_data['num_of_*']=full_data.description.apply(lambda x:x.count('*'))\n",
    "full_data['num_of_>']=full_data.description.apply(lambda x:x.count('>'))\n",
    "full_data['num_of_puncs']=full_data['num_of_#'] + full_data['num_of_!'] + full_data['num_of_$'] + full_data['num_of_*'] + full_data['num_of_>']\n",
    "full_data['puncs_ratio'] = full_data['num_of_puncs']/full_data['len_of_desc']\n",
    "full_data['upper_char_ratio'] = full_data['description'].apply(lambda x: 0 if sum([s.isalpha() for s in x])==0 else sum([s.isalpha()&s.isupper() for s in x])/ sum([s.isalpha() for s in x]))\n",
    "\n",
    "# Accuracy of location/ address\n",
    "full_data['disp_is_street'] = (full_data['display_address'] == full_data['street_address'])*1\n",
    "full_data['disp_st_addr_word_ratio'] = full_data.apply(lambda x:len(x['display_address'].split(' '))/len(x['street_address'].split(' ')), axis=1)\n",
    "\n",
    "listing_quality_vars = ['disp_is_street', 'num_of_html_tag','num_of_#','num_of_!','num_of_$', 'num_of_*',\n",
    "                        'posted_times', 'disp_st_addr_word_ratio','upper_char_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars +geo_num_vars + count_vars \\\n",
    "    + listing_vars + listing_quality_vars\n",
    "full_cat_vars = LE_vars + mean_coded_vars\n",
    "full_vars = full_num_vars + full_cat_vars\n",
    "train_x = sparse.hstack([full_data[full_vars], \n",
    "                         feature_sparse, \n",
    "                         desc_sparse, \n",
    "                         st_addr_sparse]).tocsr()[:train_size]\n",
    "train_y = full_data['target'][:train_size].values\n",
    "test_x = sparse.hstack([full_data[full_vars], \n",
    "                        feature_sparse, \n",
    "                        desc_sparse, \n",
    "                        st_addr_sparse]).tocsr()[train_size:]\n",
    "test_y = full_data['target'][train_size:].values\n",
    "\n",
    "\n",
    "full_vars = full_vars + feature_vars + desc_vars + st_addr_vars    \n",
    "print (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.05\n",
    "lgb_params['num_leaves'] = 63\n",
    "lgb_params['max_depth'] = 15\n",
    "lgb_params['min_gain_to_split '] = 1\n",
    "lgb_params['subsample'] = 0.7\n",
    "lgb_params['colsample_bytree'] = 0.7\n",
    "lgb_params['min_sum_hessian_in_leaf'] = 0.001\n",
    "lgb_params['seed']=42\n",
    "\n",
    "lgb_cv = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(train_x,\n",
    "                            label=train_y\n",
    "                            ),\n",
    "                num_boost_round=100000,\n",
    "                nfold=5,\n",
    "                stratified=True,\n",
    "                shuffle=True,\n",
    "                early_stopping_rounds=50,\n",
    "                seed=42,\n",
    "                verbose_eval=50)\n",
    "\n",
    "\n",
    "best_score = min(lgb_cv['multi_logloss-mean'])\n",
    "best_iteration = len(lgb_cv['multi_logloss-mean'])\n",
    "print ('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n",
    "\n",
    "# 0.547453"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manager performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def p25(x):\n",
    "    return np.percentile(x, 25)\n",
    "def p50(x):\n",
    "    return np.percentile(x, 50)\n",
    "def p75(x):\n",
    "    return np.percentile(x, 75)\n",
    "def nunique(x):\n",
    "    return np.size(np.unique(x))\n",
    "def max_min(x):\n",
    "    return np.max(x)-np.min(x)\n",
    "def p75_p25(x):\n",
    "    return np.percentile(x, 75)-np.percentile(x, 25)\n",
    "\n",
    "\n",
    "\n",
    "def get_group_stats(df, stat_funcs, target_column, group_column, ranking=False, ranking_pct=True):\n",
    "    aggr = df.groupby(group_column)[target_column].agg([v for v in stat_funcs.values()]).reset_index()\n",
    "    aggr.columns = [group_column] + [  target_column + '_' + k + '_by_' + group_column for k in stat_funcs.keys()]\n",
    "    aggr = df[[group_column]].merge(aggr, how='left', on=group_column)\n",
    "    \n",
    "    #rank\n",
    "    if ranking:\n",
    "        aggr[target_column + '_rank_by_' + group_column] = df.groupby(group_column)[target_column].rank(method='dense', \n",
    "                                                                                                    na_option='top',\n",
    "                                                                                                    pct=ranking_pct)\n",
    "    return aggr.drop(group_column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_funcs = {\n",
    "#     'count_unique': nunique,\n",
    "    'mean': np.mean,\n",
    "    'min': np.min,\n",
    "    'max': np.max,\n",
    "    'std': np.std,\n",
    "    'p25': p25,\n",
    "    'p50': p50,\n",
    "    'p75': p75,\n",
    "    'skew': skew,\n",
    "    'kurtosis': kurtosis,\n",
    "    'max_min': max_min,\n",
    "    'p75_p25': p75_p25\n",
    "}\n",
    "\n",
    "\n",
    "mgr_aggr = pd.DataFrame()\n",
    "for num_var in num_vars + additional_num_vars + listing_quality_vars:\n",
    "    mgr_aggr = pd.concat([mgr_aggr,\n",
    "                          get_group_stats(full_data, stat_funcs,\n",
    "                                          target_column=num_var, group_column='manager_id', ranking=False)\n",
    "                          ],\n",
    "                         axis=1\n",
    "                         )\n",
    "    \n",
    "## manager activeness\n",
    "mgr_aggr = pd.concat([mgr_aggr,\n",
    "                      get_group_stats(full_data, {'max_min': max_min, 'p75_p25': p75_p25},\n",
    "                                      target_column='created_epoch', group_column='manager_id', ranking=False)\n",
    "                      ],\n",
    "                     axis=1\n",
    "                     )\n",
    "\n",
    "mgr_aggr = pd.concat([mgr_aggr,\n",
    "                      get_group_stats(full_data, {'nunique': nunique},\n",
    "                                      target_column='created_dayofyear', group_column='manager_id', ranking=False)\n",
    "                      ],\n",
    "                     axis=1\n",
    "                     )\n",
    "\n",
    "## Buildings managed by the manager\n",
    "mgr_aggr = pd.concat([mgr_aggr,\n",
    "                      get_group_stats(full_data, {'nunique': nunique},\n",
    "                                      target_column='building_id', group_column='manager_id', ranking=False)\n",
    "                      ],\n",
    "                     axis=1\n",
    "                     )\n",
    "\n",
    "## Areas \n",
    "for aggr_col in ['geo_area_50', 'geo_area_100', 'geo_area_200']:\n",
    "    mgr_aggr = pd.concat([mgr_aggr,\n",
    "                          get_group_stats(full_data, {'nunique': nunique},\n",
    "                                          target_column=aggr_col, group_column='manager_id', ranking=False)\n",
    "                          ],\n",
    "                         axis=1\n",
    "                         )\n",
    "\n",
    "## Price fairness    \n",
    "for aggr_col in ['diff_to_mkt_price', 'ratio_to_mkt_price']:\n",
    "    mgr_aggr = pd.concat([mgr_aggr,\n",
    "                          get_group_stats(full_data, {'mean': np.mean},\n",
    "                                          target_column=aggr_col, group_column='manager_id', ranking=False)\n",
    "                          ],\n",
    "                         axis=1\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars + count_vars \\\n",
    "    + listing_vars + listing_quality_vars\n",
    "full_cat_vars = LE_vars + mean_coded_vars\n",
    "full_vars = full_num_vars + full_cat_vars\n",
    "train_x = sparse.hstack([full_data[full_vars],\n",
    "                         feature_sparse,\n",
    "                         desc_sparse,\n",
    "                         st_addr_sparse,\n",
    "                         mgr_aggr]).tocsr()[:train_size]\n",
    "train_y = full_data['target'][:train_size].values\n",
    "test_x = sparse.hstack([full_data[full_vars],\n",
    "                        feature_sparse,\n",
    "                        desc_sparse,\n",
    "                        st_addr_sparse,\n",
    "                        mgr_aggr]).tocsr()[train_size:]\n",
    "test_y = full_data['target'][train_size:].values\n",
    "\n",
    "\n",
    "full_vars = full_vars + feature_vars + desc_vars + st_addr_vars\n",
    "print(\"training data size: \", train_x.shape,\n",
    "      \"testing data size: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.05\n",
    "lgb_params['num_leaves'] = 63\n",
    "lgb_params['max_depth'] = 15\n",
    "lgb_params['min_gain_to_split '] = 1\n",
    "lgb_params['subsample'] = 0.7\n",
    "lgb_params['colsample_bytree'] = 0.7\n",
    "lgb_params['min_sum_hessian_in_leaf'] = 0.001\n",
    "lgb_params['seed']=42\n",
    "\n",
    "lgb_cv = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(train_x,\n",
    "                            label=train_y\n",
    "                            ),\n",
    "                num_boost_round=100000,\n",
    "                nfold=5,\n",
    "                stratified=True,\n",
    "                shuffle=True,\n",
    "                early_stopping_rounds=50,\n",
    "                seed=42,\n",
    "                verbose_eval=50)\n",
    "\n",
    "\n",
    "best_score = min(lgb_cv['multi_logloss-mean'])\n",
    "best_iteration = len(lgb_cv['multi_logloss-mean'])\n",
    "print ('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n",
    "# Best iteration: 306, best score: 0.531753"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar for building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_funcs = {\n",
    "#     'count_unique': nunique,\n",
    "    'mean': np.mean,\n",
    "    'min': np.min,\n",
    "    'max': np.max,\n",
    "    'std': np.std,\n",
    "    'p25': p25,\n",
    "    'p50': p50,\n",
    "    'p75': p75,\n",
    "    'skew': skew,\n",
    "    'kurtosis': kurtosis,\n",
    "    'max_min': max_min,\n",
    "    'p75_p25': p75_p25\n",
    "}\n",
    "\n",
    "\n",
    "building_aggr = pd.DataFrame()\n",
    "\n",
    "building_aggr = pd.concat([building_aggr,\n",
    "                      get_group_stats(full_data, stat_funcs,\n",
    "                                      target_column='price', group_column='building_id', ranking=False)\n",
    "                      ],\n",
    "                     axis=1\n",
    "                     )\n",
    "    \n",
    "\n",
    "## Buildings managed by the manager\n",
    "building_aggr = pd.concat([building_aggr,\n",
    "                      get_group_stats(full_data, {'nunique': nunique},\n",
    "                                      target_column='manager_id', group_column='building_id', ranking=False)\n",
    "                      ],\n",
    "                     axis=1\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars +geo_num_vars + count_vars \\\n",
    "    + listing_vars + listing_quality_vars\n",
    "full_cat_vars = LE_vars + mean_coded_vars\n",
    "full_vars = full_num_vars + full_cat_vars\n",
    "train_x = sparse.hstack([full_data[full_vars],\n",
    "                         feature_sparse,\n",
    "                         desc_sparse,\n",
    "                         st_addr_sparse,\n",
    "                         mgr_aggr,\n",
    "                         building_aggr]).tocsr()[:train_size]\n",
    "train_y = full_data['target'][:train_size].values\n",
    "test_x = sparse.hstack([full_data[full_vars],\n",
    "                        feature_sparse,\n",
    "                        desc_sparse,\n",
    "                        st_addr_sparse,\n",
    "                        mgr_aggr,\n",
    "                       building_aggr]).tocsr()[train_size:]\n",
    "test_y = full_data['target'][train_size:].values\n",
    "\n",
    "\n",
    "full_vars = full_vars + feature_vars + desc_vars + st_addr_vars\n",
    "print(\"training data size: \", train_x.shape,\n",
    "      \"testing data size: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.05\n",
    "lgb_params['num_leaves'] = 63\n",
    "lgb_params['max_depth'] = 15\n",
    "lgb_params['min_gain_to_split '] = 1\n",
    "lgb_params['subsample'] = 0.7\n",
    "lgb_params['colsample_bytree'] = 0.7\n",
    "lgb_params['min_sum_hessian_in_leaf'] = 0.001\n",
    "lgb_params['seed']=42\n",
    "\n",
    "lgb_cv = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(train_x,\n",
    "                            label=train_y\n",
    "                            ),\n",
    "                num_boost_round=100000,\n",
    "                nfold=5,\n",
    "                stratified=True,\n",
    "                shuffle=True,\n",
    "                early_stopping_rounds=50,\n",
    "                seed=42,\n",
    "                verbose_eval=50)\n",
    "\n",
    "\n",
    "best_score = min(lgb_cv['multi_logloss-mean'])\n",
    "best_iteration = len(lgb_cv['multi_logloss-mean'])\n",
    "print ('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n",
    "# Best iteration: 306, best score: 0.531753"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Location Location!!!\n",
    "\n",
    "Not all listings were created equally. Location is one of the most dominant factors when seeking a place to live. When we think of location we are not only talking about the absolute location but the relative location, e.g. proximity to facilities such as school, transportations and supermarkets. Unfortunately, these information are not provided by the dataset naively but thanks to Kaggler [Farron](https://www.kaggle.com/mmueller) who graciously shared his secret sauce which brilliantly hacked the proximity information and helped him win the second place in this competition. Here's what he did: \n",
    "> It consists of kmeans cluster of (latitude, longitude) followed by computing statistics like the ones above and cluster center distances. In order to get some proxies for PoI's in the neighborhood, I created clusters after filtering the dataset based on certain words in the descriptions. That way, I estimated coordinates for things like \"supermarket\", \"shopping\", \"subway\", \"bus\", \"health\", \"fitness\", \"park\" etc. Afterwards I created minimal distances to those locations as well as counts based on different distances cut-offs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parks\n",
    "\n",
    "The biggest challenge for replicating Faron's great idea is to figure out the appropriate number of clusters for each category. How can we do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import vincenty\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "park_listings = full_data[full_data[['description', 'features']].apply(lambda x: 'park' in x[0] or 'park' in x[1], axis=1)][['latitude', 'longitude']]\n",
    "\n",
    "park_n_clusters = 25\n",
    "kms = KMeans(n_clusters=park_n_clusters)\n",
    "kms.fit(park_listings)\n",
    "\n",
    "park_dist_data = pd.DataFrame(kms.transform(full_data[['latitude', 'longitude']]),\n",
    "                              columns = ['dist_to_park_' + str(i) for i in range(park_n_clusters)]\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars +geo_num_vars+ geo_cat_vars + count_vars \\\n",
    "    + listing_vars + listing_quality_vars  \n",
    "full_cat_vars = LE_vars + mean_coded_vars\n",
    "full_vars = full_num_vars + full_cat_vars\n",
    "train_x = sparse.hstack([full_data[full_vars],\n",
    "                         feature_sparse,\n",
    "                         desc_sparse,\n",
    "                         st_addr_sparse,\n",
    "                         mgr_aggr,\n",
    "                        building_aggr,\n",
    "                        park_dist_data]).tocsr()[:train_size]\n",
    "train_y = full_data['target'][:train_size].values\n",
    "test_x = sparse.hstack([full_data[full_vars],\n",
    "                        feature_sparse,\n",
    "                        desc_sparse,\n",
    "                        st_addr_sparse,\n",
    "                        mgr_aggr,\n",
    "                       building_aggr,\n",
    "                       park_dist_data]).tocsr()[train_size:]\n",
    "test_y = full_data['target'][train_size:].values\n",
    "\n",
    "\n",
    "full_vars = full_vars + feature_vars + desc_vars + st_addr_vars\n",
    "print(\"training data size: \", train_x.shape,\n",
    "      \"testing data size: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.05\n",
    "lgb_params['num_leaves'] = 63\n",
    "lgb_params['max_depth'] = 15\n",
    "lgb_params['min_gain_to_split '] = 1\n",
    "lgb_params['subsample'] = 0.7\n",
    "lgb_params['colsample_bytree'] = 0.7\n",
    "lgb_params['min_sum_hessian_in_leaf'] = 0.001\n",
    "lgb_params['seed']=42\n",
    "\n",
    "lgb_cv = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(train_x,\n",
    "                            label=train_y\n",
    "                            ),\n",
    "                num_boost_round=100000,\n",
    "                nfold=5,\n",
    "                stratified=True,\n",
    "                shuffle=True,\n",
    "                early_stopping_rounds=50,\n",
    "                seed=42,\n",
    "                verbose_eval=50)\n",
    "\n",
    "\n",
    "best_score = min(lgb_cv['multi_logloss-mean'])\n",
    "best_iteration = len(lgb_cv['multi_logloss-mean'])\n",
    "print ('Best iteration: %d, best score: %f' % (best_iteration, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your reference, we can use the following snippet to fine tune the optimal number clusters\n",
    "\n",
    "```python\n",
    "\n",
    "scores = []\n",
    "for park_n_clusters in (10, 15, 20, 25, 30):\n",
    "    kms = KMeans(n_clusters=park_n_clusters)\n",
    "    kms.fit(park_listings)\n",
    "\n",
    "    park_dist_data = pd.DataFrame(kms.transform(full_data[['latitude', 'longitude']]),\n",
    "                                  columns=['dist_to_park_' +\n",
    "                                      str(i) for i in range(park_n_clusters)]\n",
    "                                 )\n",
    "\n",
    "    full_num_vars = num_vars + date_num_vars + additional_num_vars + \\\n",
    "        interactive_num_vars + listing_vars + listing_quality_vars + magic_vars + \\\n",
    "        num_cat_vars + mean_coded_vars + distance_vars\n",
    "    full_cat_vars = LE_vars\n",
    "    full_vars = full_num_vars + full_cat_vars\n",
    "    train_x = sparse.hstack([full_data[full_vars],\n",
    "                             feature_sparse,\n",
    "                             desc_sparse,\n",
    "                             st_addr_sparse,\n",
    "                             mgr_aggr,\n",
    "                            park_dist_data]).tocsr()[:train_size]\n",
    "    train_y = full_data['target'][:train_size].values\n",
    "    test_x = sparse.hstack([full_data[full_vars],\n",
    "                            feature_sparse,\n",
    "                            desc_sparse,\n",
    "                            st_addr_sparse,\n",
    "                            mgr_aggr,\n",
    "                            park_dist_data]).tocsr()[train_size:]\n",
    "    test_y = full_data['target'][train_size:].values\n",
    "\n",
    "    full_vars = full_vars + feature_vars + desc_vars + st_addr_vars\n",
    "    print(\"training data size: \", train_x.shape,\n",
    "          \"testing data size: \", test_x.shape)\n",
    "\n",
    "    lgb_params = dict()\n",
    "    lgb_params['objective'] = 'multiclass'\n",
    "    lgb_params['num_class'] = 3\n",
    "    lgb_params['learning_rate'] = 0.05\n",
    "    lgb_params['num_leaves'] = 63\n",
    "    lgb_params['max_depth'] = 15\n",
    "    lgb_params['min_gain_to_split '] = 1\n",
    "    lgb_params['subsample'] = 0.7\n",
    "    lgb_params['colsample_bytree'] = 0.7\n",
    "    lgb_params['min_sum_hessian_in_leaf'] = 0.001\n",
    "    lgb_params['seed'] = 42\n",
    "\n",
    "    lgb_cv = lgb.cv(lgb_params,\n",
    "                    lgb.Dataset(train_x,\n",
    "                                label=train_y\n",
    "                                ),\n",
    "                    num_boost_round=100000,\n",
    "                    nfold=5,\n",
    "                    stratified=True,\n",
    "                    shuffle=True,\n",
    "                    early_stopping_rounds=50,\n",
    "                    seed=42,\n",
    "                    verbose_eval=100)\n",
    "\n",
    "    best_score = min(lgb_cv['multi_logloss-mean'])\n",
    "    best_iteration = len(lgb_cv['multi_logloss-mean'])\n",
    "    print('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n",
    "    scores.append([park_n_clusters, best_score])\n",
    "scores = np.array(scores)\n",
    "best_park_n_clusters = scores[:, 0][(np.argmin(scores[:, 1]))]\n",
    "print('best number of clusters: %d, best score: %f' % (best_park_n_clusters, np.min(scores[:, 1])))\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_listings = full_data[full_data[['description', 'features']].apply(lambda x: 'subway' in x[0] or 'subway' in x[1], axis=1)][['latitude', 'longitude']]\n",
    "\n",
    "subway_n_clusters = 400\n",
    "kms = KMeans(n_clusters=subway_n_clusters)\n",
    "kms.fit(subway_listings)\n",
    "\n",
    "subway_dist_data = pd.DataFrame(kms.transform(full_data[['latitude', 'longitude']]),\n",
    "                              columns = ['dist_to_subway_' + str(i) for i in range(subway_n_clusters)]\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+geo_num_vars+ geo_cat_vars + count_vars \\\n",
    "    + listing_vars + listing_quality_vars \n",
    "full_cat_vars = LE_vars + mean_coded_vars\n",
    "full_vars = full_num_vars + full_cat_vars\n",
    "train_x = sparse.hstack([full_data[full_vars],\n",
    "                         feature_sparse,\n",
    "                         desc_sparse,\n",
    "                         st_addr_sparse,\n",
    "                         mgr_aggr,\n",
    "                        building_aggr,\n",
    "                        park_dist_data,\n",
    "                        subway_dist_data]).tocsr()[:train_size]\n",
    "train_y = full_data['target'][:train_size].values\n",
    "test_x = sparse.hstack([full_data[full_vars],\n",
    "                        feature_sparse,\n",
    "                        desc_sparse,\n",
    "                        st_addr_sparse,\n",
    "                        mgr_aggr,\n",
    "                       building_aggr,\n",
    "                       park_dist_data,\n",
    "                       subway_dist_data]).tocsr()[train_size:]\n",
    "test_y = full_data['target'][train_size:].values\n",
    "\n",
    "\n",
    "full_vars = full_vars + feature_vars + desc_vars + st_addr_vars\n",
    "print(\"training data size: \", train_x.shape,\n",
    "      \"testing data size: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.05\n",
    "lgb_params['num_leaves'] = 63\n",
    "lgb_params['max_depth'] = 15\n",
    "lgb_params['min_gain_to_split '] = 1\n",
    "lgb_params['subsample'] = 0.7\n",
    "lgb_params['colsample_bytree'] = 0.7\n",
    "lgb_params['min_sum_hessian_in_leaf'] = 0.001\n",
    "lgb_params['seed']=42\n",
    "\n",
    "lgb_cv = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(train_x,\n",
    "                            label=train_y\n",
    "                            ),\n",
    "                num_boost_round=100000,\n",
    "                nfold=5,\n",
    "                stratified=True,\n",
    "                shuffle=True,\n",
    "                early_stopping_rounds=50,\n",
    "                seed=42,\n",
    "                verbose_eval=50)\n",
    "\n",
    "\n",
    "best_score = min(lgb_cv['multi_logloss-mean'])\n",
    "best_iteration = len(lgb_cv['multi_logloss-mean'])\n",
    "print ('Best iteration: %d, best score: %f' % (best_iteration, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## item2vec\n",
    "\n",
    "We can use the same idea for word2vec to embed any items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy ## Spacy is the de-facto NLP tool used by industry\n",
    "from gensim.models import FastText  \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "## Tokenize a sentence\n",
    "def seq_to_token(seq, nlp=nlp):\n",
    "    doc = nlp(str(seq).lower())\n",
    "    tokens = [token.text for token in doc if not ( token.is_space | token.is_stop|token.like_num)]\n",
    "    return tokens\n",
    "\n",
    "## Convert tokens to vector\n",
    "def tokens_to_vec(tokens, model, vec_size=10):\n",
    "    if len(tokens)==0:\n",
    "        return np.zeors(vec_size)\n",
    "    else:\n",
    "        return np.array([emb_model[token] for token in tokens]).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed building id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Generate \"sentences\"\n",
    "building_by_mgr = full_data.groupby('manager_id')['building_id'].apply(list)\n",
    "building_by_mgr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Train a fasttext model\n",
    "building_model = FastText(size=10, window=3, min_count=1, workers=16)  # instantiate\n",
    "building_model.build_vocab(sentences=building_by_mgr)\n",
    "building_model.train(sentences=building_by_mgr.values, total_examples=len(building_by_mgr.values), epochs=5)\n",
    "\n",
    "## Take a look at the embedding for building_id 8a8b08e08888819a3e745005a8cd0408\n",
    "building_model['8a8b08e08888819a3e745005a8cd0408']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Embed building ids\n",
    "building_emb = full_data['building_id'].apply(lambda x:building_model[x]).values\n",
    "building_emb = np.array([e.reshape(1,-1) for e in building_emb]).reshape(-1,10)\n",
    "building_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed manager id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_by_building = full_data.groupby('building_id')['manager_id'].apply(list)  \n",
    "manager_model = FastText(size=10, window=3, min_count=1, workers=16)\n",
    "manager_model.build_vocab(sentences=manager_by_building)\n",
    "manager_model.train(sentences=manager_by_building.values, \n",
    "                    total_examples=len(manager_by_building.values), epochs=5)\n",
    "manager_emb = full_data['manager_id'].apply(lambda x:manager_model[x]).values\n",
    "manager_emb = np.array([e.reshape(1,-1) for e in manager_emb]).reshape(-1,10)\n",
    "manager_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_by_building = full_data.groupby('building_id')['manager_id'].apply(list)  \n",
    "manager_model = FastText(size=10, window=3, min_count=1, workers=16)\n",
    "manager_model.build_vocab(sentences=manager_by_building)\n",
    "manager_model.train(sentences=manager_by_building.values, \n",
    "                    total_examples=len(manager_by_building.values), epochs=5)\n",
    "manager_emb = full_data['manager_id'].apply(lambda x:manager_model[x]).values\n",
    "manager_emb = np.array([e.reshape(1,-1) for e in manager_emb]).reshape(-1,10)\n",
    "manager_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+geo_num_vars+ geo_cat_vars + count_vars \\\n",
    "    + listing_vars + listing_quality_vars  \n",
    "full_cat_vars = LE_vars + mean_coded_vars\n",
    "full_vars = full_num_vars + full_cat_vars\n",
    "train_x = sparse.hstack([full_data[full_vars],\n",
    "                         feature_sparse,\n",
    "                         desc_sparse,\n",
    "                         st_addr_sparse,\n",
    "                         mgr_aggr,\n",
    "                        building_aggr,\n",
    "                        park_dist_data,\n",
    "                        subway_dist_data,\n",
    "                        manager_emb,\n",
    "                        building_emb]).tocsr()[:train_size]\n",
    "train_y = full_data['target'][:train_size].values\n",
    "test_x = sparse.hstack([full_data[full_vars],\n",
    "                        feature_sparse,\n",
    "                        desc_sparse,\n",
    "                        st_addr_sparse,\n",
    "                        mgr_aggr,\n",
    "                       building_aggr,\n",
    "                       park_dist_data,\n",
    "                       subway_dist_data,\n",
    "                       manager_emb,\n",
    "                    building_emb]).tocsr()[train_size:]\n",
    "test_y = full_data['target'][train_size:].values\n",
    "\n",
    "\n",
    "\n",
    "print(\"training data size: \", train_x.shape,\n",
    "      \"testing data size: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.05\n",
    "lgb_params['num_leaves'] = 63\n",
    "lgb_params['max_depth'] = 15\n",
    "lgb_params['min_gain_to_split '] = 1\n",
    "lgb_params['subsample'] = 0.7\n",
    "lgb_params['colsample_bytree'] = 0.7\n",
    "lgb_params['min_sum_hessian_in_leaf'] = 0.001\n",
    "lgb_params['seed']=42\n",
    "\n",
    "lgb_cv = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(train_x,\n",
    "                            label=train_y\n",
    "                            ),\n",
    "                num_boost_round=100000,\n",
    "                nfold=5,\n",
    "                stratified=True,\n",
    "                shuffle=True,\n",
    "                early_stopping_rounds=50,\n",
    "                seed=42,\n",
    "                verbose_eval=50)\n",
    "\n",
    "\n",
    "best_score = min(lgb_cv['multi_logloss-mean'])\n",
    "best_iteration = len(lgb_cv['multi_logloss-mean'])\n",
    "print ('Best iteration: %d, best score: %f' % (best_iteration, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The magic feature\n",
    "\n",
    "Firstly mentioned by Grand Master Silogram\n",
    "https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31765\n",
    "\n",
    "Discovered and made available to public by another Grand Master KazAnova\n",
    "https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870\n",
    "\n",
    "It may contain the information when the listing was actually created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_date = pd.read_csv(\"../input/twosigma-magic-feature/listing_image_time.csv\")\n",
    "\n",
    "image_date.columns = [\"listing_id\", \"image_time_stamp\"]\n",
    "full_data = pd.merge(full_data, image_date, on=\"listing_id\", how=\"left\")\n",
    "magic_vars = ['image_time_stamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+geo_num_vars+ geo_cat_vars + count_vars \\\n",
    "    + listing_vars + listing_quality_vars + magic_vars + price_vars\n",
    "full_cat_vars = LE_vars + mean_coded_vars\n",
    "full_vars = full_num_vars + full_cat_vars\n",
    "train_x = sparse.hstack([full_data[full_vars],\n",
    "                         feature_sparse,\n",
    "                         desc_sparse,\n",
    "                         st_addr_sparse,\n",
    "                         mgr_aggr,\n",
    "                        building_aggr,\n",
    "                        park_dist_data,\n",
    "                        subway_dist_data,\n",
    "                        manager_emb,\n",
    "                        building_emb]).tocsr()[:train_size]\n",
    "train_y = full_data['target'][:train_size].values\n",
    "test_x = sparse.hstack([full_data[full_vars],\n",
    "                        feature_sparse,\n",
    "                        desc_sparse,\n",
    "                        st_addr_sparse,\n",
    "                        mgr_aggr,\n",
    "                       building_aggr,\n",
    "                       park_dist_data,\n",
    "                       subway_dist_data,\n",
    "                       manager_emb,\n",
    "                    building_emb]).tocsr()[train_size:]\n",
    "test_y = full_data['target'][train_size:].values\n",
    "\n",
    "\n",
    "\n",
    "print(\"training data size: \", train_x.shape,\n",
    "      \"testing data size: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.05\n",
    "lgb_params['num_leaves'] = 63\n",
    "lgb_params['max_depth'] = 15\n",
    "lgb_params['min_gain_to_split '] = 1\n",
    "lgb_params['subsample'] = 0.7\n",
    "lgb_params['colsample_bytree'] = 0.7\n",
    "lgb_params['min_sum_hessian_in_leaf'] = 0.001\n",
    "lgb_params['seed']=42\n",
    "\n",
    "lgb_cv = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(train_x,\n",
    "                            label=train_y\n",
    "                            ),\n",
    "                num_boost_round=100000,\n",
    "                nfold=5,\n",
    "                stratified=True,\n",
    "                shuffle=True,\n",
    "                early_stopping_rounds=50,\n",
    "                seed=42,\n",
    "                verbose_eval=50)\n",
    "\n",
    "\n",
    "best_score = min(lgb_cv['multi_logloss-mean'])\n",
    "best_iteration = len(lgb_cv['multi_logloss-mean'])\n",
    "print ('Best iteration: %d, best score: %f' % (best_iteration, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text embedding (optional)\n",
    "\n",
    "The pretrained FastText embedding can be downloaded and installed using the following commands:\n",
    "\n",
    "```shell\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
    "!python -m spacy init-model en ../embedding/crawl-300d-2M --vectors-loc ../embedding/crawl-300d-2M.vec.zip\n",
    "```\n",
    "\n",
    "Then in Python:\n",
    "\n",
    "```Python\n",
    "import spacy  \n",
    "nlp_fasttext = spacy.load(\"../embedding/crawl-300d-2M\")\n",
    "# nlp_glove = spacy.load(\"en_core_web_lg\")\n",
    "def seq_to_vec(seq, nlp, dim=300):\n",
    "    doc = nlp(str(seq))\n",
    "    vec = np.array(\n",
    "        [\n",
    "            token.vector\n",
    "            for token in doc\n",
    "            if not ( token.is_space | token.is_oov)\n",
    "        ]\n",
    "    ).mean(axis=0)\n",
    "    if isinstance(vec, np.ndarray):\n",
    "        return vec\n",
    "    else:\n",
    "        return np.zeros((dim))\n",
    "desc_emb = np.array([v for v in full_data[\"description\"].fillna('').apply(lambda x:seq_to_vec(x, nlp_fasttext)).values])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model tuning\n",
    "## LightGBM\n",
    "### Manual tuning\n",
    "We will manually tune LightGBM paramters one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "greater_is_better = False\n",
    "\n",
    "lgb_metric = 'multi_logloss'\n",
    "default_lgb_params = {}\n",
    "default_lgb_params[\"objective\"] = \"multiclass\"\n",
    "default_lgb_params[\"num_class\"] = 3\n",
    "default_lgb_params[\"learning_rate\"] = 0.05\n",
    "default_lgb_params[\"metric\"] = lgb_metric\n",
    "default_lgb_params[\"bagging_freq\"] = 1\n",
    "default_lgb_params[\"seed\"] = 1234\n",
    "\n",
    "params_lgb_space = {}\n",
    "params_lgb_space['feature_fraction'] = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "params_lgb_space['num_leaves'] = [3, 7, 15, 31, 63, 127]\n",
    "params_lgb_space['max_depth'] = [3, 7, 10, 15, 31, -1]\n",
    "params_lgb_space['min_gain_to_split'] = [0, 0.1, 0.3, 1, 1.5, 2, 3]\n",
    "params_lgb_space['bagging_fraction'] = [0.2, 0.4, 0.6, 0.8, 1]\n",
    "params_lgb_space['min_sum_hessian_in_leaf'] = [0, 0.0001, 0.001, 0.1, 1, 3, 10]\n",
    "params_lgb_space['lambda_l2'] = [0, 0.01, 0.1, 1, 10, 100]\n",
    "params_lgb_space['lambda_l1'] = [0, 0.01, 0.1, 1, 10]\n",
    "\n",
    "\n",
    "best_lgb_params = copy.copy(default_lgb_params)\n",
    "\n",
    "for p in params_lgb_space:\n",
    "    print (\"\\n Tuning parameter %s in %s\" % (p, params_lgb_space[p]))\n",
    "\n",
    "    params = best_lgb_params\n",
    "    scores = []    \n",
    "    for v in params_lgb_space[p]:\n",
    "        print ('\\n    %s: %s' % (p, v), end=\"\\n\")\n",
    "        params[p] = v\n",
    "        lgb_cv = lgb.cv(params,\n",
    "                lgb.Dataset(train_x,\n",
    "                            label=train_y\n",
    "                            ),\n",
    "                num_boost_round=100000,\n",
    "                nfold=5,\n",
    "                stratified=False,\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=False)\n",
    "        if greater_is_better:\n",
    "            best_lgb_score = min(lgb_cv['%s-mean' % (lgb_metric)])\n",
    "        else:\n",
    "            best_lgb_score = min(lgb_cv['%s-mean' % (lgb_metric)])\n",
    "        best_lgb_iteration = len(lgb_cv['%s-mean' % (lgb_metric)])\n",
    "        print (', best_score: %f, best_iteration: %d' % (best_lgb_score, best_lgb_iteration))\n",
    "        scores.append([v, best_lgb_score])\n",
    "    # best param value in the space\n",
    "    best_param_value = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][0]\n",
    "    best_param_score = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][1]\n",
    "    best_lgb_params[p] = best_param_value\n",
    "    print (\"Best %s is %s with a score of %f\" %(p, best_param_value, best_param_score))\n",
    "\n",
    "print ('\\n Best manually tuned parameters:', best_lgb_params)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('\\n Best manually tuned parameters:', best_lgb_params)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated tuning with Bayesian Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "def lgb_evaluate(\n",
    "                 num_leaves,\n",
    "                 max_depth,\n",
    "                 min_sum_hessian_in_leaf,\n",
    "                 min_gain_to_split,\n",
    "                 feature_fraction,\n",
    "                 bagging_fraction,\n",
    "                 lambda_l2,\n",
    "                 lambda_l1\n",
    "                 ):\n",
    "    params = dict()\n",
    "    params['objective'] = 'multiclass'\n",
    "    params['num_class'] = 3\n",
    "    params['learning_rate'] = 0.05\n",
    "    params['seed'] = 1234\n",
    "    params['num_leaves'] = int(num_leaves)  \n",
    "    params['max_depth'] = int(max_depth) \n",
    "    params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "    params['min_gain_to_split'] = min_gain_to_split    \n",
    "    params['feature_fraction'] = feature_fraction\n",
    "    params['bagging_fraction'] = bagging_fraction\n",
    "    params['bagging_freq'] = 1\n",
    "    params['lambda_l2'] = lambda_l2\n",
    "    params['lambda_l1'] = lambda_l1   \n",
    "    params[\"metric\"] = lgb_metric\n",
    "\n",
    "    lgb_cv = lgb.cv(params,\n",
    "            lgb.Dataset(train_x,\n",
    "                        label=train_y\n",
    "                        ),\n",
    "            num_boost_round=100000,\n",
    "            nfold=5,\n",
    "            stratified=False,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=-1)\n",
    "    if greater_is_better:\n",
    "        best_lgb_score = min(lgb_cv['%s-mean' % (lgb_metric)])\n",
    "    else:\n",
    "        best_lgb_score = min(lgb_cv['%s-mean' % (lgb_metric)])\n",
    "    best_lgb_iteration = len(lgb_cv['%s-mean' % (lgb_metric)])\n",
    "    print (', best_score: %f, best_iteration: %d' % (best_lgb_score, best_lgb_iteration))\n",
    "\n",
    "    return -best_lgb_score\n",
    "\n",
    "\n",
    "lgb_BO = BayesianOptimization(lgb_evaluate, \n",
    "                             {\n",
    "                              'num_leaves': (10, 20),\n",
    "                              'max_depth': (2, 20),\n",
    "                              'min_sum_hessian_in_leaf': (5, 15),\n",
    "                              'min_gain_to_split': (0,0),\n",
    "                              'feature_fraction': (0.2, 0.4),\n",
    "                              'bagging_fraction': (0.8,1),\n",
    "                              'lambda_l2': (5, 15),\n",
    "                              'lambda_l1': (0.1, 5)\n",
    "                             }\n",
    "                            )\n",
    "## I use 5, 20 to save time but you may want to change it to larger numbers,e.g. 8, 30 \n",
    "lgb_BO.maximize(init_points=5, n_iter=20) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show top 5 best tuned parameters. Comparing to the manually tuned results, which one works better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_BO_scores = pd.DataFrame([p['params'] for p in lgb_BO.res])\n",
    "lgb_BO_scores['score'] = [p['target'] for p in lgb_BO.res]\n",
    "lgb_BO_scores = lgb_BO_scores.sort_values(by='score',ascending=False)\n",
    "lgb_BO_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the model with smaller learning rate\n",
    "Now let's validate the model again but with a smaller learning rate(0.01 as compared to 0.05) and see the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_best_params = lgb_BO_scores.T.to_dict().get(lgb_BO_scores.index.values[0])\n",
    "lgb_best_params['objective'] = 'multiclass'\n",
    "lgb_best_params['learning_rate'] = 0.01 ## from 0.05 to 0.01\n",
    "lgb_best_params['num_class'] = 3\n",
    "lgb_best_params['seed'] = 1234\n",
    "lgb_best_params['metric'] = lgb_metric\n",
    "lgb_best_params['bagging_freq'] = 1\n",
    "\n",
    "lgb_best_params['num_leaves'] = int(lgb_best_params['num_leaves'])\n",
    "lgb_best_params['max_depth'] = int(lgb_best_params['max_depth'])\n",
    "\n",
    "print(lgb_best_params)\n",
    "\n",
    "lgb_cv = lgb.cv(lgb_best_params,\n",
    "        lgb.Dataset(train_x,\n",
    "                    label=train_y\n",
    "                    ),\n",
    "        num_boost_round=100000,\n",
    "        nfold=5,\n",
    "        stratified=True,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=100)\n",
    "if greater_is_better:\n",
    "    best_lgb_score = min(lgb_cv['%s-mean' % (lgb_metric)])\n",
    "else:\n",
    "    best_lgb_score = min(lgb_cv['%s-mean' % (lgb_metric)])\n",
    "best_lgb_iteration = len(lgb_cv['%s-mean' % (lgb_metric)])\n",
    "print (', best_score: %f, best_iteration: %d' % (best_lgb_score, best_lgb_iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Apparently the model performs better with a smaller learning rate however it also took more iterations (longer time) to converge.\n",
    "\n",
    "Now let's retrain the model with the learning rate 0.01 as well as the tuned iterations(num_boost_round) and generate the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.train(lgb_best_params,\n",
    "                  lgb.Dataset(train_x,\n",
    "                              label=train_y\n",
    "                              ),\n",
    "                  num_boost_round=best_lgb_iteration)\n",
    "preds = model.predict(test_x)\n",
    "sub_lgb_df = pd.DataFrame(preds, columns=[\"low\", \"medium\", \"high\"])\n",
    "sub_lgb_df[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_lgb_df.to_csv(\"../output/sub_lgb_auto_tuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
